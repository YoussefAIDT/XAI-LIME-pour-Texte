# üîç Explicabilit√© de l'IA (XAI) : Analyse de Sentiment avec LIME

![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=for-the-badge&logo=python&logoColor=white)
![Hugging Face](https://img.shields.io/badge/Transformers-DistilBERT-yellow?style=for-the-badge&logo=huggingface&logoColor=white)
![LIME](https://img.shields.io/badge/XAI-LIME-green?style=for-the-badge)
![License](https://img.shields.io/badge/License-Academic-lightgrey?style=for-the-badge)

> **Mini-projet acad√©mique sur l'interpr√©tabilit√© des mod√®les de Deep Learning (NLP).**

---

## üë• Auteurs
Ce projet a √©t√© r√©alis√© par :
* **ES-SAAIDI Youssef**
* **ZEMMAHI Zakariae**
* *Date : D√©cembre 2025*

---

## üìù Description du Projet

L'objectif de ce projet est d'ouvrir la "bo√Æte noire" des mod√®les de traitement du langage naturel (NLP). Nous utilisons la m√©thode **LIME (Local Interpretable Model-agnostic Explanations)** pour comprendre pourquoi un mod√®le de Deep Learning classe une critique de film comme **Positive** ou **N√©gative**.

Nous avons impl√©ment√© un pipeline complet qui :
1.  Utilise un mod√®le **DistilBERT** pr√©-entra√Æn√© (`distilbert-base-uncased-finetuned-sst-2-english`).
2.  Effectue des pr√©dictions sur le dataset de critiques de films **IMDb**.
3.  G√©n√®re des explications visuelles montrant l'impact de chaque mot sur la d√©cision finale.

---

## üß† Concepts Cl√©s

| Concept | Description |
| :--- | :--- |
| **Black Box** | Mod√®les complexes (R√©seaux de neurones) dont le processus de d√©cision interne est opaque. |
| **LIME** | Technique qui approxime le mod√®le complexe par un mod√®le lin√©aire simple localement pour expliquer une pr√©diction unique. |
| **Perturbation** | G√©n√©ration de variantes de la phrase en masquant des mots pour tester la sensibilit√© du mod√®le. |

---

## üõ†Ô∏è Stack Technique

* **Langage :** Python 3
* **Mod√®le :** Hugging Face Transformers
* **Biblioth√®ques Principales :**
    * `lime` : Algorithme d'explicabilit√©.
    * `transformers` : Chargement du mod√®le BERT.
    * `datasets` : Chargement des donn√©es IMDb.
    * `torch` : Backend Deep Learning.
    * `matplotlib` / `numpy` : Visualisation et calculs matriciels.

---

## üöÄ Installation et Utilisation

### 1. Cloner le projet
```bash
git clone [[https://github.com/VOTRE_NOM_UTILISATEUR/NOM_DU_REPO.git](https://github.com/YoussefAIDT/XAI-LIME-pour-Texte)]
cd XAI-LIME-pour-Texte
````

### 2\. Installer les d√©pendances

Il est recommand√© d'utiliser un environnement virtuel. Installez les paquets n√©cessaires avec :

```bash
pip install -q transformers datasets lime torch scikit-learn matplotlib
```

### 3\. Ex√©cuter le Notebook

Le c≈ìur du projet se trouve dans le fichier Jupyter Notebook. Lancez-le via :

```bash
jupyter notebook mini_projet_lime_texte.ipynb
```

*Note : Ce projet est √©galement optimis√© pour s'ex√©cuter directement sur Google Colab.*

-----

## üìä Visualisation des R√©sultats

Le projet produit des visualisations intuitives pour expliquer les pr√©dictions.

### Exemple d'analyse

**Phrase d'entr√©e :** *"The movie was good but the ending was boring"*

**Interpr√©tation LIME :**

  * üü© **Mots Verts (Positifs) :** `good` (Score: +0.32)
  * üü• **Mots Rouges (N√©gatifs) :** `boring` (Score: -0.45)

Cela confirme que le mod√®le d√©tecte correctement les adjectifs porteurs de sentiment.

-----

## üîÆ Limitations et Perspectives

  * **Instabilit√© :** Les explications peuvent varier l√©g√®rement entre deux ex√©cutions dues √† l'√©chantillonnage al√©atoire de LIME.
  * **Performance :** Le processus de perturbation n√©cessite plusieurs secondes par phrase.
  * **Piste future :** Comparer les r√©sultats avec la m√©thode **SHAP** (Shapley Additive Explanations) pour valider la robustesse.

-----

## üìú Licence

Ce projet est r√©alis√© dans un cadre acad√©mique (Master/Ing√©nierie) et est ouvert √† toute utilisation p√©dagogique.

```
